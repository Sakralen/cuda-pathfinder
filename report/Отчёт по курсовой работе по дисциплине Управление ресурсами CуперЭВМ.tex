\documentclass[a4paper, 12pt]{article}
\usepackage[russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[unicode, pdftex]{hyperref}
\graphicspath{{Resource/}}
\usepackage[left=3cm,right=1cm,top=2cm,bottom=2cm]{geometry}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{pdfpages}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{255,255,255}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{orange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{language=C, 
        extendedchars=\true,
        style=mystyle}

\addto\captionsrussian{\def\refname{Список источников}}

%\DeclareMathSizes{12}{30}{16}{12}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\thispagestyle{empty}
	\begin{center}
		
		{\LargeСАНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ ПОЛИТЕХНИЧЕСКИЙ УНИВЕРСИТЕТ ПЕТРА ВЕЛИКОГО}
		
		\vspace{0.2cm}
		{\largeИнститут Компьютерных наук и технологий}
		
		\vspace{0.2cm}
		{\largeВысшая школа искусственного интеллекта}
		
		\vspace{0.2cm}
		{\largeНаправление 02.03.01 Математика и компьютерные науки} 
		
		\vspace{5cm}
		{\largeОтчёт по курсовой работе по дисциплине <<Управление ресурсами CуперЭВМ>>}
		
		\vspace{0.5cm}
		{\large\textbf{<<Решение задачи построения кратчайшего пути движения робота по полигону с использованием аппаратно-программной платформы NVIDIA CUDA>>}}

		\vspace{0.5cm}
		{\largeГруппа: 3530201/00101}
	
	\end{center}
	
	\vspace{5cm}
	\parbox{9cm}{
		{\largeСтудент:}\hspace{1.9cm}\rule{3cm}{1pt} 
		
		\vspace{2cm}
		{\largeПреподаватель:}\hspace{0.5cm}\rule{3cm}{1pt} 
	}
	\parbox{9cm}{
	{\largeСпасов Григорий Ефимович}
	
	\vspace{2cm}
	{\largeКурочкин Михаил Александрович}
	}

	\vspace{2cm}
	\begin{flushright}
		{\large{<<}\rule{0.75cm}{1pt}{>>}\rule{3cm}{1pt} {20}\rule{0.75cm}{1pt} {г.}}
	\end{flushright}
	
	\vspace{1.25cm}	
	\begin{center}
		{\largeСанкт-Петербург -- 2023}
	\end{center}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\newpage
	\tableofcontents
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \newpage
	\section*{Введение}
    \addcontentsline{toc}{section}{Введение}
    Суперкомпьютер -- это высокопроизводительная компьютерная система, способная выполнять огромное количество вычислений за короткий промежуток времени. Он состоит из сотен и тысяч процессоров, объединенных вместе для параллельной обработки данных.
    
    Суперкомпьютеры используются в различных областях науки и промышленности. Они позволяют проводить более точные и сложные моделирования, исследования климата и погоды, расчеты в физике и астрономии, анализ геномов, разработку новых материалов, аэродинамические и гидродинамические расчеты, машинное обучение и многое другое. Суперкомпьютеры являются неотъемлемым инструментом для научных и инженерных исследований, где требуется высокая вычислительная мощность.
    
    Одним из примеров суперкомпьютерного центра в Санкт-Петербурге является суперкомпьютерный центр "Политехнический". Он базируется на технологиях NVIDIA и предоставляет вычислительные ресурсы для решения различных научных и инженерных задач. В частности, для решения практических задач на графическом процессоре суперкомпьютера NVIDIA используется платформа CUDA. CUDA (Compute Unified Device Architecture) - это архитектура, разработанная NVIDIA, которая позволяет разработчикам программ использовать вычислительные возможности графических процессоров (GPU) для выполнения широкого спектра вычислительных задач. CUDA обеспечивает эффективную параллельную обработку данных и значительное ускорение вычислений на суперкомпьютерах, использующих графические процессоры NVIDIA.
    
    В данной курсовой работе представлен обзор аппаратно-программной платформы \linebreak NVIDIA CUDA, её архитектурные особенности, модель вычислений, модель памяти и особенности работы с графическими процессорами NVIDIA. Также представлен обзор вычислительных систем и средств суперкомпьютерного центра «Политехнический», его состав и характеристики вычислительных узлов.
    
    В качестве практической части курсовой работы был описан и реализован алгоритм построения пути движения робота по полигону с использованием аппаратно-программной платформы NVIDIA CUDA. Полученная программа была запущена на вычислительном узле суперкомпьютерного центра «Политехнический».
    
    Был поставлен эксперимент, в ходе которого выяснялась зависимость времени
    исполнения программы от параметров распараллеливания на GPU.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \clearpage
    \section{Постановка задачи}
    В рамках выполнения курсовой работы были поставлены следующие задачи:
    \begin{enumerate}
        \item Изучить аппаратно-программную платформу NVIDIA CUDA: её архитектуру, вычислительные возможности, устройство памяти, модель вычислений;
        \item Изучить и применить на практике технологию программирования CUDA C;
        \item Изучить состав и характеристики вычислительных средств суперкомпьютерного центра «Политехнический», изучить и применить технологию подключения к СКЦ;
        \item Реализовать с использованием технологии CUDA C алгоритм построения кратчайшего пути движения робота по полигону;
        \item Поставить эксперимент для выяснения зависимости времени выполнения программы от выбора модели памяти и размера исходного массива;
        \item Проанализировать результаты эксперимента.
    \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \newpage
    \section{Аппаратно-программная платформа NVIDIA CUDA}
    \subsection{Архитектура NVIDIA CUDA}
    Архитектура NVIDIA CUDA (Compute Unified Device Architecture) - это программная и аппаратная платформа, разработанная компанией NVIDIA, которая позволяет использовать графические процессоры (GPU) для параллельных вычислений и ускорения обработки данных \cite{cuda_tech}.
    
    Основной принцип архитектуры CUDA заключается в использовании мощности вычислений графического процессора для выполнения параллельных задач, работающих вместе с центральным процессором (CPU) компьютера. Графический процессор состоит из множества вычислительных ядер, которые могут выполнять однотипные задачи одновременно, независимо друг от друга. Это позволяет обрабатывать большие объемы данных и решать сложные вычислительные задачи с высокой производительностью.
    
    Основные характеристики архитектуры NVIDIA CUDA:
    \begin{itemize}
        \item Язык программирования: CUDA поддерживает язык программирования CUDA C/C++, который предоставляет расширения для работы с параллельными вычислениями на GPU. 
        \item Модель исполнения: CUDA использует модель исполнения, основанную на понятии ядер (kernels). Ядро (kernel) - это функция, которая выполняется параллельно на множестве потоков на графическом процессоре.
        \item Модель памяти: CUDA предоставляет шесть типов памяти, включая глобальную память, разделяемую память (shared memory) и константную память (constant memory). Каждый тип памяти имеет свои особенности и используется для различных целей, таких как обмен данными между потоками и сохранение постоянных данных.
    \end{itemize}
    \subsection{Вычислительные возможности NVIDIA CUDA}
    Параметр "compute capability"\,(вычислительная способность) в NVIDIA CUDA используется для описания возможностей и характеристик конкретной архитектуры графического процессора \cite{cuda_guide}. Он определяет, какие функции и возможности доступны для программ, компилируемых с использованием CUDA.
    
    Вычислительная способность обозначается числовым значением, состоящим из двух цифр, разделенных точкой (например, 6.1 или 7.0). Первая цифра обозначает основную версию архитектуры, а вторая цифра указывает на подверсию.
    
    Вычислительная способность зависит от конкретной архитектуры GPU и определяет следующие характеристики:
    \begin{itemize}
        \item Количество и тип вычислительных ядер: Вычислительная способность определяет количество ядер SM (Streaming Multiprocessors) на графическом процессоре и тип архитектуры ядра, такой как NVIDIA's Fermi, Kepler, Maxwell, Pascal, Volta, Turing или Ampere. Каждая архитектура имеет свои особенности и функции.
        \item Размер разделяемой памяти и регистров: Вычислительная способность также определяет доступный объем разделяемой памяти (shared memory) и количество доступных регистров для каждого потока выполнения на графическом процессоре. Эти ресурсы могут использоваться для ускорения вычислений и обмена данными между потоками.
        \item Поддержка определенных функций и инструкций: Вычислительная способность определяет, какие функции и инструкции доступны для использования в программе CUDA. Новые архитектуры могут включать новые инструкции и возможности, которые не доступны в старых версиях.
        \item Производительность и энергоэффективность: Вычислительная способность также связана с производительностью и энергоэффективностью архитектуры GPU. Более новые версии архитектур часто имеют улучшенную производительность и более эффективное использование энергии.
    \end{itemize}
    
    В СК «Политехник - РСК Торнадо», на котором проводились вычисления, установлены графические процессоры Nvidia Tesla K40. Они имеют compute capability версии 3.5, ниже приведены их основные характеристики.
    \begin{itemize}
    \item Количество одновременно выполняющихся ядер: 32;
    \item Максимальная размерность сетки блоков: 3-х мерная;
    \item Максимальная x-составляющая сетки блоков: $2^{31 - 1}$;
    \item Максимальная y- и z-составляющая сетки блоков: 65535;
    \item Максимальная размерность блока: 3-х мерный;
    \item Максимальная x- и y-составляющая блока: 1024;
    \item Максимальная z-составляющая блока: 64;
    \item Максимальное количество нитей в блоке: 1024;
    \item Размер варпа: 32;
    \item Максимальное количество одновременно выполняющихся блоков на SM: 16;
    \item Максимальное количество варпов на SM: 64;
    \item Максимальное количество нитей на SM: 2048;
    \item Количество 32-битных регистров на SM: 64K;
    \item Максимальное количество 32-битных регистров на исполняемый блок: 64K;
    \item Максимальное количество 32-битных регистров на исполняемую нить: 255; 
    \item Максимальный объем разделяемой памяти: 48 KB;
    \item Максимальное объем разделяемой памяти на блок: 48 KB;
    \item Объем константной памяти: 64 KB;
    \item Рабочий объем кэша константой памяти на SM: 8 KB;
     \end{itemize}
    \subsection{Потоковая модель}
    Вычислительная архитектура CUDA основана на концепции одна команда на множество данных (Single Instruction Multiple Data, SIMD) и понятии мультипроцессора.
    
    Концепция \textbf{SIMD} подразумевает, что одна инструкция позволяет одновременно обработать множество данных. 
    
    \textbf{Мультипроцессор} — это многоядерный SIMD процессор, позволяющий в каждый определенный момент времени выполнять на всех ядрах только одну инструкцию. Каждое ядро мультипроцессора скалярное, т.е. оно не поддерживает векторные операции в чистом виде.
    
    Также в вычислительной архитектуре CUDA важны понятия устройство и хост.
    
    \textbf{Устройство (device)} - видеоадаптер, поддерживающий драйвер CUDA, или другое специализированное устройство, предназначенное для исполнения программ, использующих CUDA.
    
    
    \textbf{Хост (host)} - программа в обычной оперативной памяти компьютера, использующую CPU и выполняющую управляющие функции по работе с устройством.
    
    Логически устройство можно представить как набор мультипроцессоров (см. Рис. \ref{device}) плюс драйвер CUDA.
    
    \begin{figure}[h!]
    	\centering\includegraphics[scale=0.75]{logik.png}
         \caption{Архитектура устройства}	
     \label{device}
    \end{figure}

    \clearpage
    \subsection{Устройство памяти}
    В CUDA выделяют шесть видов памяти (рис. \ref{memory}). Это регистры, локальная, глобальная, разделяемая, константная и текстурная память.
    Такое обилие обусловлено спецификой видеокарты и первичным ее предназначением, а также стремлением разработчиков сделать систему как можно дешевле, жертвуя в различных случаях либо универсальностью, либо скоростью. Подробно каждый вид памяти будет рассмотрен в следующем разделе.
    \begin{figure}[h!]
    	\centering\includegraphics[scale=0.25]{memory.jpg}
         \caption{Виды памяти}	
     \label{memory}
    \end{figure}
    \subsection{Модели памяти}
    В CUDA выделяют шесть видов памяти:
    \begin{enumerate}
        \item \textbf{Регистры} -- память, в которой по возможности компилятор старается размещать все локальные переменные функций. Доступ к 
    таким переменным осуществляется с максимальной скоростью. В текущей архитектуре на один 
    мультипроцессор доступно 8192 32-разрядных регистра. 
    \item \textbf{Локальная память} --  в случаях, когда локальные данные процедур занимают слишком большой размер, или компилятор не может 
    вычислить для них некоторый постоянный шаг при обращении, он может поместить их в локальную память. Этому может способствовать, например, приведение указателей для типов разных размеров. Физически локальная память является аналогом глобальной памяти, и работает с той же скоростью.
    \item \textbf{Глобальная память} -- самый большой объём памяти, доступный для всех МП на видеочипе (размер от 256Mбайт до 4Гбайт). Основная особенность -  возможность 
    произвольной адресации. Однако глобальная память работает очень медленно, не кэшируется. Поэтому количество обращений к глобальной памяти следует минимизировать. Глобальная память необходима в основном для сохранения результатов работы программы перед отправкой 
    их на хост (в обычную память DRAM). 
    \item \textbf{Разделяемая память} -- некэшируемая, но быстрая память. Ее и рекомендуется использовать как 
    управляемый кэш. На один мультипроцессор доступно всего 16KB разделяемой памяти. 
    Отличительной чертой разделяемой памяти является то, что она адресуется одинаково для всех задач 
    внутри блока. Отсюда следует, что ее можно использовать для обмена данными между потоками 
    только одного блока.
    \item \textbf{Константная память} --  размер 
    составляет всего 64 Kбайт (на все устройство). Константная память кэшируется, поэтому доступ в общем случае достаточно быстрый. Кэш существует в единственном экземпляре для одного мультипроцессора, а значит, общий для всех задач внутри блока. 
    Константная память очень удобна в использовании. Можно размещать в ней данные любого типа и читать их 
    при помощи простого присваивания. Однако из-за её небольшого объема имеет смысл 
    хранить лишь небольшое количество часто используемых данных.
    \item \textbf{Текстурная память} --  блок памяти, доступный для чтения всеми МП. Кэшируется. Медленная, как глобальная - сотни тактов задержки при 
    отсутствии данных в кэше. Имеет очень важное свойство пространственной локальности. При вычислении на модели в виде матрицы, где соседние
    элементы взаимодействуют друг с другом, часто возникает необходимость обращаться к элементам окрестности элемента матрицы. С точки зрения адресной арифметики элементы окрестности какого-либо элемента матрицы не расположены в памяти рядом друг с другом. Для ускорения вычисления вышеописанного вида применяется текстурная память, которая позволяет кэшировать данные по свойству их пространственной, а не адресной локальности.
    \end{enumerate}
    \subsection{Модель вычислений на GPU}
    В основе модели вычислений на GPU лежит понятие сетки блоков.  На Рис. \ref{threads} ядро обозначено как Kernel. Все потоки, выполняющие это ядро, объединяются в блоки (Block), а блоки, в свою очередь, объединяются в сетку (Grid).

    \clearpage
    \begin{figure}[h!]
    	\centering\includegraphics[scale=0.75]{threads.png}
         \caption{Организация потоков}	
     \label{threads}
    \end{figure}
    
    Вычисление шейдерной программы (ядра) на матрице данных распределяется по блокам, ответственным за отдельно взятый участок матрицы данных. Блоки образуют одно- двух- трёх мерную сетку.
    
    В свою очередь выполнение программы на участке матрицы данных блока выполняется параллельно в нитях, из которых состоит блок. Нить — это экземпляр выполняемого ядра на отдельно взятом наборе входных данных. 
    
    Не все нити блока запускаются на выполнение одновременно: они разбиваются на варпы\footnote{Варп (warp) - группа одинаковых потоков, исполняющих одну и ту же инструкцию одновременно на графическом процессоре NVIDIA CUDA.} по 32 нити. 

    Когда потоки внутри варпа выполняют доступы к памяти, например, чтение или запись в глобальную память, они часто обращаются к смежным адресам памяти. Кэширование варпов используется для ускорения этих доступов к памяти --  когда один поток в варпе выполняет доступ к памяти, данные загружаются из глобальной памяти в специальный кэш варпа. Если другие потоки в том же варпе позже обратятся к тем же адресам памяти, они смогут получить данные непосредственно из кэша варпа, избегая доступа к глобальной памяти. Это позволяет существенно сократить задержку на чтение и запись в память. Если потоки в варпе обращаются к памяти в произвольном порядке или часто совершают прыжки в адресном пространстве, эффективность кэширования варпа может снизиться.
    
    При разбиении задачи на блоки стоит учитывать ограничения аппаратной архитектуры CUDA, которые заключаются в том, что на одном SM может выполнятся несколько блоков, но один блок может выполнятся только на одном SM.
    
    Очевидно, что одновременное выполнение одинаковых инструкций на нитях в варпе возможно только если в шейдерной программе не используются операторы ветвления.В случае, если операторы ветвления используются, то варп разделяется на две группы нитей, которые поочерёдно выполняют инструкции одной и другой ветки условного оператора.

    \subsection{Интерфейс программирования CUDA C}
     \subsubsection{Спецификаторы типов функций} 
     Программисту доступны следующие спецификаторы функций:

\begin{itemize}
    \item  \_\_device\_\_ объявляет функцию, которая: выполняется на GPU; может быть вызвана только с GPU.
    \item \_\_global\_\_ объявляет функцию (ядро), которая: выполняется на GPU; может быть вызвана только с ЦПУ.
    \item \_\_host\_\_ объявляет функцию, которая: выполняется на ЦПУ; может быть вызвана только с ЦПУ. Объявление функции со спецификатором \_\_host\_\_ эквивалентно ее объявлению без спецификаторов \_\_device\_\_, \_\_global\_\_ или \_\_host\_\_; в обоих случаях функция компилируется только для ЦПУ.
\end{itemize}

\subsubsection{Правила и ограничения при объявлении функций}
\begin{itemize}
    \item Спецификатор \_\_host\_\_ может использоваться совместно со 
    спецификатором \_\_device\_\_. В данном случае функция компилируется как для ЦПУ, так и для GPU. 
    \item Функции со спецификатором \_\_device\_\_ и \_\_global\_\_ не могут содержать объявления статических переменных, не поддерживают переменное число аргументов. 
    \item Функции со спецификатором \_\_global\_\_ не поддерживают рекурсию. Ранние версии CUDA и устройств также не поддерживают рекурсию для функций со спецификатором \_\_device\_\_. 
    \item Спецификаторы \_\_global\_\_ и \_\_host\_\_ не могут использоваться совместно в объявлении функции.  
    \item Функции со спецификатором \_\_global\_\_ (ядра) должны возвращать тип void.   
    \item Вызов функции со спецификатором \_\_global\_\_ является асинхронным. Это означает, что возврат управления осуществляется до того, как выполнение функции на GPU закончится. 
    \item Параметры функции со спецификатором \_\_global\_\_ передаются через разделяемую память.
\end{itemize}

\subsubsection{Спецификаторы типов переменных} 
Программисту доступны следующие спецификаторы типов переменных:
\begin{itemize}
\item \_\_device\_\_ объявляет переменную, которая: - размещается на GPU в глобальном пространстве памяти; - имеет время жизни, равное времени жизни приложения; - доступна из всех потоков, выполняемых на GPU; - доступна из основной программы через библиотеки времени выполнения.

\item \_\_constant\_\_ объявляет переменную, которая: - размещается на GPU в константном пространстве памяти; - имеет время жизни, равное времени 
жизни приложения; - доступна из всех потоков, выполняемых на GPU; - 
доступна из основной программы через библиотеки времени выполнения.

\item \_\_shared\_\_ объявляет переменную, которая: - размещается на GPU в разделяемой памяти блока потоков (для каждого блока потоков будет 
создан свой экземпляр переменной); - имеет время жизни, равное времени жизни блока потоков; - доступна из потоков, принадлежащих блоку потоков.
\end{itemize}

\subsubsection{При объявлении переменных действуют следующие правила и 
ограничения:}

\begin{itemize}
\item Указанные спецификаторы неприменимы к формальным параметрам 
функций, а также к локальным переменным функций, исполняемых на ЦПУ.

\item Переменные со спецификаторами \_\_shared\_\_ и \_\_constant\_\_ 
подразумевают статическое хранение. 

\item Переменные со спецификаторами \_\_device\_\_, \_\_shared\_\_ и \_\_constant\_\_ не могут быть объявлены с помощью ключевого слова extern. Исключение 
составляет так называемая динамически распределяемая разделяемая 
память.

\item Переменные со спецификаторами \_\_device\_\_ и \_\_constant\_\_ должны быть 
объявлены в глобальном пространстве имен. 

\item Переменные со спецификатором \_\_constant\_\_ могут быть 
инициализированы только с ЦПУ через функции времени выполнения. 
\item Переменные со спецификатором \_\_shared\_\_ не могут инициализироваться 
при объявлении.

\item Автоматическая переменная, объявленная в выполняемой на GPU 
функции, без использования вышеперечисленных спецификаторов обычно 
размещается в регистрах. Однако в некоторых случаях компилятор может 
размещать ее в локальной памяти. Обычно это происходит, когда 
объявляются большие структуры или массивы, которые могут потребовать 
слишком большого количества пространства памяти регистров, либо 
объявляются массивы, для которых компилятор не может определить, 
являются ли они индексированными с использованием константных 
величин.
 
\item Указатели в коде, который выполняется на GPU, поддерживаются до тех 
пор, пока компилятор способен определить: указывают ли они на 
пространство общей памяти или на глобальное пространство памяти. В 
противном случае могут использоваться лишь указатели на память, 
выделенную в глобальном пространстве памяти GPU. 
\end{itemize}

Необходимо отметить, что на всех GPU с поддержкой CUDA скорость работы разделяемой памяти существенно (на 2 порядка) превосходит скорость работы глобальной памяти 1, поэтому одной из основных техник 
оптимизации является размещение интенсивно используемых данных в 
разделяемой памяти. Кроме того, данный вид памяти открывает 
возможности для эффективной кооперации потоков одного блока. 

\subsubsection{Встроенные переменные} 
В приложении на языке CUDA C (в функциях, 
исполняемых на GPU) доступны следующие встроенные переменные:
\begin{itemize}
    \item gridDim Переменная типа dim3, содержит текущую размерность решетки; 
    \item blockIdx Переменная типа uint3, содержит индекс блока внутри решетки; 
    \item blockDim Переменная типа dim3, содержит размерность блока потоков;
    \item threadIdx Переменная типа uint3, содержит индекс потока внутри блока; 
    \item warpSize Переменная типа int, содержит размер варпа в количестве 
    потоков.
\end{itemize}
Указанные встроенные переменные предназначены только для чтения и не 
могут быть модифицированы вызывающей программой.

 \subsubsection{Конфигурирование исполнения ядер}
Любой вызов функции со 
спецификатором \_\_global\_\_ (ядра) должен определять конфигурацию 
исполнения для данного вызова. Конфигурация выполнения задает 
размерность решетки и блоков, которые будут использоваться для 
исполнения функции на GPU. - 1 На устройствах архитектуры Fermi появились 
L1/L2 кэши для глобальной памяти. В случае попадания в кэш скорость 
доступа примерно равна скорости доступа к разделяемой памяти. 
Конфигурация определяется с помощью выражения специального вида <<>> 
между именем функции и списком ее аргументов, где:
\begin{itemize}
\item grid Переменная типа dim3, которая определяет размерность и размер 
сетки, так что grid.x × grid.y × grid.z равно числу блоков потоков, которые 
будут запущены (в ранних версиях CUDA требовалось, чтобы grid.z всегда 
было равно 1).
\item block Переменная типа dim3, которая определяет размерность и размер 
каждого блока потоков, block.x × block.y × block.z равно числу потоков на 
блок. 
\item size Переменная типа size\_t, определяет число байт в разделяемой памяти, 
которое динамически выделяется на блок для этого вызова в добавление к 
статически выделенной памяти. Данная динамически выделяемая память 
используется переменными, объявленными как внешние массивы. Аргумент 
\item size является необязательным, значение по умолчанию равно 0.
\item stream Переменная типа cudaStream\_t, определяет CUDA-поток (в смысле 
потоков на ЦПУ), ассоциированный с выполнением ядра. Механизм CUDA-
потоков применяется для обеспечения асинхронной работы и использования 
нескольких GPU.
\end{itemize}
 Данный аргумент является необязательным, для приложений, не 
использующих CUDA-потоки в явном виде, используется значение по 
умолчанию. 
Таким образом, обязательными частями конфигурации исполнения являются 
лишь первые две: количество блоков и размер блока.

\subsubsection{Kernel}
Kernel является самым важным элементом расширения CUDA C, 
которое позволяет запустить код, написанный в ядре, параллельно. С точки 
зрения синтаксиса C/C++ ядро вызывается довольно нетипичной 
конструкцией:
kernel<<<gridSize, blockSize, \newline sharedMemSize, cudaStream>>>().
\begin{itemize}

\item gridSize - размер сетки блоков, задается типом dim3, который задает 
количество блоков по каждой из осей OX, OY, OZ.
\item blockSize - размер блока в потоках, также задается типом dim3.
sharedMemSize - размер разделяемой памяти для каждого блока.
\item cudaStream - переменная cudaStream\_t, задающая поток, в котором 
будет произведен вызов.
\end{itemize}

    \clearpage
    \subsection{Компиляция программы}
     Компиляция кода с использованием CUDA происходит под управлением программы nvcc.
    
     Компилятор nvcc производит отделение кода, выполняющегося на GPU, от кода, выполняющегося на CPU, и компиляцию кода, выполняющегося на GPU, в бинарное представление (объекты cubin). Для кода, выполняющегося на CPU, полностью поддерживаются все возможности языка C++. Для кода, выполняющего на GPU, поддерживается подмножество языка C++ (с расширениями языка CUDA C). Как исключение из правил использования C++, указатель на void не может быть присвоен указателю на данные другого типа без явного приведения типа. 
    
    Для более точной настройки компиляции и оптимизации программы CUDA под конкретные требования и характеристики аппаратного обеспечения используются директивы компилятора nvcc. Некоторые из них:
    \begin{itemize}
        \item \_\_noinline\_\_ - данный cпецификатор указывает компилятору не встраивать код функции (если это возможно). Тело функции должно содержаться с том же файле, откуда происходит вызов функции. Компилятор может не учитывать спецификатор \_\_noinline\_\_ для функций, среди параметров которых встречаются указатели, и для функций с большим списком параметров.
        \item \#pragma unroll N - данная директива используется для того, чтобы явно управлять развертыванием того или иного цикла. Директива должна быть помещена в коде непосредственно перед циклом. Дополнительный параметр директивы (N) показывает, сколько раз цикл должен быть развернут.
        \item \#pragma unroll 1 - данная директива указывает компилятору на то, что цикл не должен быть развернут. Если величина N не определена, то цикл полностью разворачивается, если его число повторений равно константе; в противном случае цикл не разворачивается вообще. 
    \end{itemize}
    \subsection{Планировщик задач}
    В NVIDIA CUDA планировщик задач, также известный как планировщик варпов (warp scheduler), играет важную роль в управлении выполнением инструкций на графическом процессоре (GPU). 
    
    Основные особенности планировщика задач варпов в NVIDIA CUDA:
    \begin{itemize}
    \item Варп: Варп представляет собой группу потоков (threads), которые исполняют одинаковую инструкцию одновременно на разных ядрах CUDA. Варпы в CUDA имеют фиксированный размер, который зависит от конкретной архитектуры GPU, например, в архитектуре NVIDIA Turing варпы состоят из 32 потоков.
    \item Планирование: Планировщик задач варпов определяет, какие варпы и в каком порядке будут исполняться на графическом процессоре. Он осуществляет динамическое переключение между варпами для использования ресурсов GPU максимально эффективным образом. Планировщик может выбирать варпы на основе различных факторов, таких как доступность данных, зависимости инструкций и т.д.
    \item Параллельность варпа: Инструкции внутри одного варпа исполняются параллельно на разных ядрах CUDA. Это позволяет достичь массовой параллельности и высокой производительности при выполнении вычислительных задач на GPU.
    \item Управление зависимостями: Планировщик задач варпов обрабатывает зависимости между инструкциями, чтобы обеспечить правильный порядок выполнения. Если инструкции в одном варпе имеют зависимости по данным или по управлению, планировщик может переключиться на другой варп и исполнять его, чтобы избежать простоя выполнения и максимизировать использование ресурсов.
    \item Многопоточность и дивергенция: Планировщик задач варпов обрабатывает множество варпов параллельно, чтобы обеспечить высокую многопоточность и эффективное использование ресурсов GPU. Однако, если инструкции внутри варпа различаются (например, из-за условных операторов), происходит дивергенция, и некоторые потоки в варпе могут ожидать, пока другие завершат выполнение.
    \end{itemize}
    
    В целом, планировщик задач варпов в NVIDIA CUDA играет ключевую роль в организации параллельного выполнения инструкций на графическом процессоре, обеспечивая высокую производительность и эффективное использование вычислительных ресурсов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \newpage
    \section{Суперкомпьютерный центр «Политехнический»}
    \subsection{Состав}
    СКЦ располагает следующими высокопроизводительными системами \cite{scc}:
    \begin{itemize}
    \item "Политехник - РСК Торнадо";
    \item "Политехник - РСК Торнадо" с ускорителями NVIDIA K-40;
    \item "Политехник - РСК Петастрим";
    \item Файловая система Lustre с системой хранения данных Xyratex ClusterStor6000.
    \end{itemize}
    \subsection{Характеристики}
    \begin{itemize}
        \item 612 узлов кластера "Политехник - РСК Торнадо" (далее tornado)
        \begin{itemize}
            \item 2 x Intel Xeon CPU E5-2697 v3 @ 2.60GHz
            \item 64G RAM
        \end{itemize}
        \item 56 узлов кластера "Политехник - РСК Торнадо" с ускорителями NVIDIA K-40
        \begin{itemize}
            \item 2 x Intel Xeon CPU E5-2697 v3 @ 2.60GHz
            \item 64G RAM
            \item 2 x Nvidia Tesla K40x 12G GDDR
        \end{itemize}
        \item 288 узлов вычислителя с ультравысокой многопоточностью "Политехник - РСК Петастрим" 
        \begin{itemize}
            \item 1 x Intel Xeon Phi 5120D @ 1.10GHz
            \item 8G RAM
        \end{itemize}
    \end{itemize}
    
    Все доступные узлы используют сеть 56Gbps FDR Infiniband в качестве
    интерконнекта. Также, на всех узлах доступна параллельная файловая система Lustre объёмом около 1 ПБ.
    
    По умолчанию пользователям предоставляется доступ к узлам tornado. Доступ к
    остальным типам узлов предоставляется по запросу.
    
    \subsection{Технология подключения}
    Подключение к суперкомпьютеру возможно при наличии логина и ключа доступа, которые выдаются сотрудниками суперкомпьютерного центра. Ключ заменяет использование пароля для доступа к суперкомпьютеру. 
    
    Вход в SSH по публичному ключу (без пароля) очень удобен и безопасен. Сотрудниками СКЦ при регистрации создаётся пара «публичный ключ — приватный ключ». Публичный ключ копируется на компьютер с сервером SSH, то есть на компьютер, к которому будет осуществляться подключение и на котором будут выполнятся команды. 
    Пользователю отдается приватный ключ.
    
    Удаленное подключение к суперкомпьютеру (вычислительному кластеру) осуществляется через головной узел по протоколу SSH по адресу login1.hpc.spbstu.ru, порт 22. Для подключения необходимо воспользоваться утилитой MobaXterm. 

    Алгоритм действий при выполнений данной курсовой работы для подключения к СКЦ:
    \begin{itemize}
        \item От сотрудников СКЦ были получены учетные данные: логин tm4u8 и файл "spasov\_ge"\, - ключ в формате OpenSSH;
        \item Была создана новая сессия в MobaXterm, полученные данные были введены в настройках сессии (Рис. \ref{session_settings});
    \end{itemize}
        
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.5]{connection_settings.jpg}
        \caption{Настройка сессии MobaXterm}	
        \label{session_settings}
    \end{figure}

    В результате было осуществлено подключение к СКЦ по протоколу SSH (Рис. \ref{connection_success}).

    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.25]{connection_success.jpg}
        \caption{Использование утилиты MobaXterm}	
        \label{connection_success}
    \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \newpage
    \section{Постановка решаемой практической задачи}
    В рамках курсовой работы была поставлена следующая практическая вычислительная
    задача, которую надо решить с помощью ресурсов СКЦ:
    \vskip 0.3cm
    \par{\bf Вариант 5.1}
    \vskip 0.3cm
    \par {\bf Дано:} 
    \begin{itemize}
    \item массив Р - полигон (nxn) целых чисел;
    \item P1(x1,y1), P2(x2,y2) - точки на полигоне;
    \end{itemize}
    \vskip 0.3cm
    \par {\bf Необходимо:} 
    \begin{itemize}
    \item построить L - траекторию минимальной длины, соединяющую P1 и P2.
    \end{itemize}
    
    \vskip 0.3cm
    \par {\bf Ограничения:}
    \begin{itemize}
    \item L не включает точки полигона со значениями P(i,j) < 0;
    \item  Значения P(i,j)  заданы по случайному распределению в диапазоне -1, 10.
    \end{itemize}
    
    \subsection{Алгоритм решения задачи}
    \textbf{Алгоритм построения L-траектории} 
    
    В качестве алгоритма построения L-траектории, соединяющую точки P1 и P2, был выбран алгоритм Ли. 
    
    Алгоритм Ли, также известный как волновой алгоритм, является алгоритмом поиска кратчайшего пути между двумя точками на планарном графе. Он основан на методе распространения волны из начальной точки к конечной точке, пройдя через все доступные пути.
    
    Его использование хорошо подходит для параллельных вычислений по нескольким причинами:
    \begin{enumerate}
        \item Каждая ячейка в графе зависит только от своих соседей. Поэтому можно легко разделить граф на различные регионы и распределить их между несколькими вычислительными узлами.
        \item Алгоритм Ли имеет хорошо определенный порядок обновления ячеек - на каждом шаге каждая ячейка обновляется только один раз. Благодаря этому легко осуществлять синхронизацию между вычислительными узлами.
    \end{enumerate}
    
    \textbf{Входные данные:}
    \begin{enumerate}
        \item Планарный граф, представленный в виде матрицы. Каждая ячейка матрицы может быть либо свободной, либо заблокированной - принимать значение -1.
        \item Начальная точка P1(x1, y1) - координаты точки, с которой начинается поиск.
        \item Конечная точка P2(x2, y2) - координаты точки, которую необходимо достичь.
    \end{enumerate}
    
    \textbf{Выходные данные:} \\
    L-траектория между начальной P1 и конечной P2 точками, представленный в виде последовательности координат ячеек.
    
    \textbf{Ограничения:}
    \begin{enumerate}
        \item Граф должен быть планарным, то есть представлять собой сетку без пересекающихся ребер.
        \item Допускается только движение вверх, вниз, влево и вправо, без диагональных перемещений.
        \item Граф должен быть ориентированным или невзвешенным, то есть каждая ячейка может быть либо свободной, либо заблокированной - имеет значение -1.
    \end{enumerate} 
    
    \textbf{Шаги алгоритма:}
    \begin{enumerate}
        \item Создать пустую матрицу того же размера, что и исходная матрица полигона. Эта матрица будет использоваться для хранения информации о расстоянии от начальной точки до каждой доступной ячейки.
        \item Установить значение начальной точки P1 в 1 в созданной матрице.
        \item Создать список, который будет хранить координаты ячеек, через которые проходит волна распространения.
        \item Добавить начальную точку P1 в список.
        \item Пока список не пуст или пока не достигнута конечная точка P2, выполнить следующие шаги:
        \begin{itemize}
            \item Извлечь текущую ячейку из списка.
            \item Получить список соседних ячеек (вверх, вниз, влево, вправо) текущей ячейки.
            \item Для каждой соседней ячейки проверить ее значение - ячейка свободная (имеет значение 0) или заблокированная(имеет значение -1). Также проверить, что в ячейке отсутствует значение в матрице.
            \item Если соседняя ячейка является свободной и не имеет значения расстояния, установить значение расстояния от начальной точки до этой ячейки, увеличивая значение расстояния текущей ячейки на 1, и добавить эту ячейку в список.
            \item Повторить предыдущий шаг для каждой соседней ячейки.
        \end{itemize}
        \item Если значение конечной точки P2 в матрице равно 0, значит, путь не найден. В противном случае, начиная с конечной точки P2, восстановить путь, перемещаясь к соседним ячейкам с уменьшающимся значением расстояния до начальной точки P1.
        \item Вернуть найденный путь - это и будет L-траектория.
    \end{enumerate}
    
    На Рис.\ref{lee} изображен пример работы алгоритма Ли построения L-траектории, соединяющей две точки. Красным цветом отмечены узлы графа, запрещенные для движения. Зеленым цветом - начальная точка, синим цветом - конечная точка. Фиолетовым цветом обозначена получившаяся траектория после действия алгоритма.

    \clearpage
    \begin{figure}[h!]
        \centering\includegraphics[scale=0.5]{lee-wave-propagation.pdf}
        \caption{Пример работы алгоритма построения L-траектории}	
        \label{lee}
    \end{figure}
        
    \subsection{Метод распараллеливания алгоритма}
    Для распараллеливания алгоритма Ли на графическом процессоре (GPU) можно использовать методы, которые позволяют одновременно вычислять значения в нескольких клетках на каждом шаге распространения волны.
    
    Одним из таких методов является параллельное выполнение алгоритма на уровне волн (wavefront level parallelism). Этот метод основан на том, что волна распространяется по графу последовательно, но значения в клетках на фронте волны можно вычислять параллельно.
    
    Основные шаги для распараллеливания алгоритма Ли на GPU:
    
    \begin{enumerate}
        \item Граф необходимо разделить на подграфы или регионы, каждый из которых будет обрабатываться отдельным блоком (thread block) на GPU. Каждый блок будет отвечать за вычисление значений в своем регионе.
        \item Исходные данные графа, включая начальную и конечную точки, загружаются на требуемую (глобальную и локальную/разделяемую) память GPU.
        \item Параллельное выполнение шагов алгоритма:
        \begin{enumerate}
            \item Выполняется первый шаг алгоритма, где вычисляются значения для клеток на расстоянии 1 от начальной точки. Каждый блок будет отвечать за вычисление значений в своем регионе.
            \item После завершения шага блоки синхронизируются, чтобы убедиться, что значения для клеток на расстоянии 1 уже доступны.
            \item Последовательно выполняются оставшиеся шаги алгоритма (2, 3, 4 и так далее), где каждый шаг вычисляет значения для клеток на определенном расстоянии от начальной точки. Каждый блок будет отвечать за вычисление значений в своем регионе на текущем шаге.
            \item Блоки синхронизируются после завершения каждого шага, чтобы убедиться, что значения для клеток на текущем расстоянии уже доступны.
        \end{enumerate}
        \item Когда алгоритм завершен и все значения расстояний вычислены, можно восстановить кратчайший путь, перемещаясь от конечной точки к начальной по ячейкам с уменьшающимся значением расстояния.
    \end{enumerate}
    
    Распараллеливание алгоритма Ли на GPU позволяет эффективно использовать вычислительные ресурсы графического процессора, особенно при работе с большими графами или при необходимости обработки множества запросов на поиск пути одновременно.
 
    \subsection{Результат работы программы}
     На Рис. \ref{unsuccessfull-trace} - Рис. \ref{successfull-trace} представлены \textit{примеры} графических выводов результата работы программы на полигоне размером 16x16. Буквой S обозначена начальная точка P1, буквой D обозначена конечная точка P2. Символом \# обозначены контуры точки, запрещенные для движения. Символом @ обозначена получившаяся в результате работы алгоритма L-траектория.
    
    На Рис.\ref{unsuccessfull-trace} представлен пример ситуации, в которой невозможно построить L-траекторию. На Рис. \ref{successfull-trace} предоставлен пример успешного построения минимальной L-траектории.

    На Рис. \ref{measurements} представлен пример вывода программой результатов измерения с варьирующимися значениями размера полигона, количества блоков в сетке и количества нитей в блоке. Вывод трассировки пути был отключен.

    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.65]{unsuccessfull-trace.jpg}
        \caption{Результат работы программы на поле 16x16 при \linebreak отсутствии возможности построить L-траекторию}	
        \label{unsuccessfull-trace}
    \end{figure}

    \clearpage
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.65]{successfull-trace.jpg}
        \caption{Результат работы программы на поле 16x16 \linebreak при успешном построении L-траектории}	 
        \label{successfull-trace}
    \end{figure}

    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.65]{measurements.jpg}
        \caption{Вывод программой измеренных значений}	
        \label{measurements}
    \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \clearpage
    \section{Описание эксперимента}
    \textbf{Цель эксперимента:} оценка зависимости времени решения задачи от объема обрабатываемых данных и параметров запуска ядер GPU.
    \medskip \par
    \textbf{Изменяемые параметры:} 
    \begin{itemize}
        \item используемая модель памяти: глобальная, /*разделяемая*/;
        \item размер исходного массива (размер полигона): $10^4$, $5\cdot10^4$, $10^5$;
        \item число блоков: 2, 4, 16, 32, 64, 128;
        \item число потоков в блоке: 2, 4, 16, 32, 64, 128.
    \end{itemize}
    
    \textbf{Методы измерения, системные средства и инструментальные средства:}
    
    Для измерения времени исполнения программы на GPU используется механизм, предоставляемый NVIDIA CUDA для работы с событиями - cudaEvent. 
    
    Для измерения времени исполнения программы на GPU с использованием cudaEvent  потребуются следующие шаги:
    \begin{enumerate}
        \item Необходимо создать два объекта cudaEvent: один для события начала (startEvent) и один для события окончания (endEvent). Это можно сделать с помощью функций cudaEventCreate().
        \item Поместить вызов cudaEventRecord() с объектом startEvent перед кодом, время выполнения которого необходимо измерить. В данном эксперименте измеряется время выполнения трех функций: генерации поля, построения всех возможных траекторий пути на поле, обратной трассировки пути.
        \item Разместить вызов cudaEventRecord() с объектом endEvent после выполнения кода, время выполнения которого необходимо измерить.
        \item Важно добавить вызов cudaEventSynchronize() перед извлечением времени, чтобы убедиться, что все операции на GPU завершены.
        \item Необходимо извлечь время между двумя событиями с помощью функции \linebreak cudaEventElapsedTime() посредством передачи указателя на переменную типа float, в которую будет записан результат измерения.
        \item Важно не забыть освободить память, занятую объектами cudaEvent, с помощью функций cudaEventDestroy().
    \end{enumerate}
    
    После выполнения данного алгоритма переменная elapsedTime содержит время выполнения кода на GPU в миллисекундах. Время решения задачи вычислялось как сумма времени выполнения трех функций: генерации поля, построения всех возможных траекторий пути между двумя точками на поле, обратной трассировки пути.
    \medskip \par
    \textbf{Число испытаний:}
    
    Было проведено по 10 испытаний для каждого уникального набора данных. Суммарное количество испытаний -- 960.
    
    \subsection{Результаты эксперимента}
    Были проведены испытания для разных параметров: используемой модели памяти, размера полигона ($10^4$, $5\cdot10^4)$, $10^5$), числа блоков (2, 4, 8, 32) и числа потоков в блоках (2, 4, 8, 32). При использовании числа блоков и числа потоков в блоке больших 32, программа выполнялась более 2 часов и завершалась с ошибкой.
    На Табл.\ref{tab1} - Табл.\ref{tab6} представлены результаты времени выполнения программы.
    \clearpage
    \begin{table}[h!]
    \centering
\begin{tabular}{|l|llll|}
\hline
 & \multicolumn{4}{l|}{Размер блоков, MxM} \\ \hline
Размер гридов, NxN & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{16} & 32 \\ \hline
2 & \multicolumn{1}{l|}{30.04} & \multicolumn{1}{l|}{36.64} & \multicolumn{1}{l|}{9.48} & 6.11 \\ \hline
4 & \multicolumn{1}{l|}{14.77} & \multicolumn{1}{l|}{8.76} & \multicolumn{1}{l|}{3.14} & 1.38 \\ \hline
16 & \multicolumn{1}{l|}{1.78} & \multicolumn{1}{l|}{1.01} & \multicolumn{1}{l|}{0.76} & 0.94 \\ \hline
32 & \multicolumn{1}{l|}{1.45} & \multicolumn{1}{l|}{0.65} & \multicolumn{1}{l|}{0.47} & 1.00 \\ \hline
\end{tabular}
\caption{Время работы алгоритма на графе $10^4$ клеток, глобальная память (мс)}
\label{tab1}
\end{table}

    \begin{table}[h!]
    \centering
\begin{tabular}{|l|llll|}
\hline
 & \multicolumn{4}{l|}{Размер блоков, MxM} \\ \hline
Размер гридов, NxN & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{16} & 32 \\ \hline
2 & \multicolumn{1}{l|}{391.70} & \multicolumn{1}{l|}{371.25} & \multicolumn{1}{l|}{136.75} & 64.72 \\ \hline
4 & \multicolumn{1}{l|}{89.16} & \multicolumn{1}{l|}{55.00} & \multicolumn{1}{l|}{30.67} & 87.32 \\ \hline
16 & \multicolumn{1}{l|}{25.34} & \multicolumn{1}{l|}{9.39} & \multicolumn{1}{l|}{3.84} & 6.74 \\ \hline
32 & \multicolumn{1}{l|}{6.21} & \multicolumn{1}{l|}{7.88} & \multicolumn{1}{l|}{2.28} & 2.57 \\ \hline
\end{tabular}
\caption{Время работы алгоритма на графе $5\cdot10^4$ клеток, глобальная память (мс)}
\label{tab2}
\end{table}

    \begin{table}[h!]
    \centering
\begin{tabular}{|l|llll|}
\hline
 & \multicolumn{4}{l|}{Размер блоков, MxM} \\ \hline
Размер гридов, NxN & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{16} & 32 \\ \hline
2 & \multicolumn{1}{l|}{327.71} & \multicolumn{1}{l|}{444.24} & \multicolumn{1}{l|}{182.91} & 194.31 \\ \hline
4 & \multicolumn{1}{l|}{204.39} & \multicolumn{1}{l|}{202.93} & \multicolumn{1}{l|}{69.53} & 150.67 \\ \hline
16 & \multicolumn{1}{l|}{63.46} & \multicolumn{1}{l|}{26.75} & \multicolumn{1}{l|}{14.29} & 30.91 \\ \hline
32 & \multicolumn{1}{l|}{11.32} & \multicolumn{1}{l|}{13.71} & \multicolumn{1}{l|}{9.03} & 6.20 \\ \hline
\end{tabular}
\caption{Время работы алгоритма на графе $10^5$ клеток, глобальная память (мс)}
\label{tab3}
\end{table}

    \begin{table}[h!]
    \centering
\begin{tabular}{|l|llll|}
\hline
 & \multicolumn{4}{l|}{Размер блоков, MxM} \\ \hline
Размер гридов, NxN & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{16} & 32 \\ \hline
2 & \multicolumn{1}{l|}{21.76} & \multicolumn{1}{l|}{39.22} & \multicolumn{1}{l|}{10.01} & 6.66 \\ \hline
4 & \multicolumn{1}{l|}{6.19} & \multicolumn{1}{l|}{4.47} & \multicolumn{1}{l|}{3.68} & 1.63 \\ \hline
16 & \multicolumn{1}{l|}{2.17} & \multicolumn{1}{l|}{1.12} & \multicolumn{1}{l|}{0.69} & 0.86 \\ \hline
32 & \multicolumn{1}{l|}{1.27} & \multicolumn{1}{l|}{0.66} & \multicolumn{1}{l|}{0.45} & 1.95 \\ \hline
\end{tabular}
\caption{Время работы алгоритма на графе $10^4$ клеток, разделяемая память (мс)}
\label{tab4}
\end{table}

    \begin{table}[h!]
    \centering
\begin{tabular}{|l|llll|}
\hline
 & \multicolumn{4}{l|}{Размер блоков, MxM} \\ \hline
Размер гридов, NxN & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{16} & 32 \\ \hline
2 & \multicolumn{1}{l|}{571.13} & \multicolumn{1}{l|}{375.41} & \multicolumn{1}{l|}{120.64} & 59.97 \\ \hline
4 & \multicolumn{1}{l|}{98.67} & \multicolumn{1}{l|}{54.19} & \multicolumn{1}{l|}{30.73} & 56.98 \\ \hline
16 & \multicolumn{1}{l|}{17.69} & \multicolumn{1}{l|}{8.36} & \multicolumn{1}{l|}{4.25} & 5.89 \\ \hline
32 & \multicolumn{1}{l|}{7.44} & \multicolumn{1}{l|}{6.18} & \multicolumn{1}{l|}{1.54} & 2.59 \\ \hline
\end{tabular}
\caption{Время работы алгоритма на графе $5\cdot10^4$ клеток, разделяемая память (мс)}
\label{tab5}
\end{table}

\clearpage
    \begin{table}[h!]
    \centering
\begin{tabular}{|l|llll|}
\hline
 & \multicolumn{4}{l|}{Размер блоков, MxM} \\ \hline
Размер гридов, NxN & \multicolumn{1}{l|}{2} & \multicolumn{1}{l|}{4} & \multicolumn{1}{l|}{16} & 32 \\ \hline
2 & \multicolumn{1}{l|}{880.10} & \multicolumn{1}{l|}{785.11} & \multicolumn{1}{l|}{270.19} & 158.88 \\ \hline
4 & \multicolumn{1}{l|}{212.56} & \multicolumn{1}{l|}{87.48} & \multicolumn{1}{l|}{69.53} & 151.37 \\ \hline
16 & \multicolumn{1}{l|}{65.75} & \multicolumn{1}{l|}{37.45} & \multicolumn{1}{l|}{17.05} & 42.50 \\ \hline
32 & \multicolumn{1}{l|}{14.80} & \multicolumn{1}{l|}{17.93} & \multicolumn{1}{l|}{7.95} & 8.86 \\ \hline
\end{tabular}
\caption{Время работы алгоритма на графе $10^5$ клеток, разделяемая память (мс)}
\label{tab6}
\end{table}

\subsection{Анализ результатов эксперимента}
При увеличении размерности сеток, среднее время выполнения испытания уменьшается, что соответствует теоретическим обоснованиям. 

При увеличении количества нитей в блоке с 2 до 16, как с использованием глобальной памяти, так и разделяемой, время выполнения испытания уменьшается до минимального; но при последующем увеличении нитей до 32 время снова увеличивается -- это объясняется накладными расходами на создание и вызов нитей, не используемых программой (выходящих за полигон). Таким образом, можно сделать вывод о том, что увеличение количества нитей в блоке не всегда приводит к увеличению производительности; существует оптимальное количество нитей в блоке, зависящее от размера полигона.

При использовании разделяемой памяти вместо глобальной, среднее время выполнения программы почти не изменяется. Это можно объяснить особенностью реализации программы: количество используемых локальных переменных в функциях, исполняемых на графическом ускорителе, мало; компиляция программы с данными переменными, отмеченными модификатором разделяемой памяти, не увеличивает производительность этих функций.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \clearpage
    \section*{Заключение}
    \addcontentsline{toc}{section}{Заключение}

    В рамках курсовой работы была изучена технология параллельного программирования
    на основе архитектуры Nvidia CUDA. 
    
    Для задачи нахождения минимальной траектории на полигоне между двумя точками был разработан  алгоритм, реализованный программно на языке CUDA C. Программа была запущена на
    ресурсах суперкомпьютерного центра «Политехнический». Для запуска использовался
    узел типа «Торнадо» с видеокартой NVIDIA Tesla K40X. Запуск программы проводился
    на одном узле с использованием одной видеокарты.

    Количество комбинаций данных размера полигона, количества блоков в сетке и количества нитей в блоке равняется 96 (48 для экспериментов с использованием глобальной памяти, 48 --  с использованием разделяемой памяти). Каждый эксперимент был запущен 10 раз для получения среднего времени эксперимента. Таким образом, всего было запущено 960 экспериментов. 

    Результаты экспериментов подтверждают теоретические данные: при увеличении размерности сеток, среднее время выполнения испытания уменьшается. Увеличение количества нитей в блоке не всегда приводит к увеличению производительности; существует оптимальное количество нитей в блоке, зависящее от размера полигона. При использовании разделяемой памяти вместо глобальной, среднее время выполнения реализованной программы почти не изменяется. При использовании числа блоков и числа потоков в блоке больших 32, программа выполнялась более 2 часов и завершалась с ошибкой, что может говорить либо об особенностях используемого графического ускорителя, либо о неоптимальной программной реализации алгоритма для работы с такими характеристиками сеток и блоков.

    Программа компилировалась на сервере СКЦ <<Политехнический>> с использованием компилятора NVCC 10.1.
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \newpage
    \begin{thebibliography}{}
	\addcontentsline{toc}{section}{Список источников}
        \bibitem{cuda_tech} Эдвард Кэндрот, Джейсон Сандерс <<Технология CUDA в примерах. Введение в программирование графических процессоров>> -- ДМК-Пресс, 2018 г. -- 232 c.
        \bibitem{cuda_guide} CUDA Programming Guide 12.1 (\url{http://developer.download.nvidia.com/compute/cuda/1_1/NVIDIA_CUDA_Programming_Guide_1.1.pdf}).
        \bibitem{scc} Краткое руководство пользователя вычислителей «Политехник - РСК Торнадо» и «Политехник - РСК Петастрим».
	\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
